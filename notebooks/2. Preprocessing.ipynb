{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/21 00:33:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/21 00:33:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    " SparkSession.builder.appName(\"project 1\")\n",
    " .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    " .config(\"spark.executor.memory\",\"6G\")\n",
    " .config(\"spark.driver.memory\",\"6G\")\n",
    " .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    " .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    " .config('spark.driver.maxResultSize', '2048m')\n",
    " .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Yellow_Taxi_Jan = spark.read.parquet('../data/raw/2023-01.parquet')\n",
    "Green_Taxi_Jan = spark.read.parquet('../data/raw/green_tripdata_2023-01.parquet')\n",
    "FHV_Jan = spark.read.parquet('../data/raw/fhv_tripdata_2023-01.parquet')\n",
    "FHV_HV_Jan = spark.read.parquet('../data/raw/fhvhv_tripdata_2023-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips recorded by Yellow Taxi:  3066766\n",
      "Number of trips recorded by Green Taxi:  68211\n",
      "Number of trips recorded by FHV:  1114320\n",
      "Number of trips recorded by FHV_hv:  18479031\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of trips recorded by Yellow Taxi: \", Yellow_Taxi_Jan.count())\n",
    "print(\"Number of trips recorded by Green Taxi: \", Green_Taxi_Jan.count())\n",
    "print(\"Number of trips recorded by FHV: \", FHV_Jan.count())\n",
    "print(\"Number of trips recorded by FHV_hv: \", FHV_HV_Jan.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Data Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding new features by transforming the exist features\n",
    "def new_feature(sdf):\n",
    "    return sdf.withColumn('datetime', date_format('tpep_pickup_datetime', 'yyyy-MM-dd'))\\\n",
    "        .withColumn('duration', unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime'))\\\n",
    "        .withColumn('hourly_earn', F.col('total_amount')/(unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime'))*3600)\\\n",
    "        .withColumn('Month', date_format('tpep_pickup_datetime', 'MM').cast('int'))\\\n",
    "        .withColumn('Hour', date_format('tpep_pickup_datetime', 'HH').cast('int'))\\\n",
    "        .withColumn('Weekend', dayofweek('tpep_pickup_datetime').isin(1,7))\\\n",
    "        .withColumn('Airport', (F.col('airport_fee') > 0).cast('BOOLEAN'))\\\n",
    "        .withColumn('Congestion',(F.col('congestion_surcharge') > 0).cast('BOOLEAN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting unusual instance\n",
    "def feature_clean(sdf):\n",
    "    return sdf.where((F.col('duration') > 60) & (F.col('duration') < 3600*5))\\\n",
    "        .where((F.col('trip_distance') < 280) & (F.col('trip_distance') > 0))\\\n",
    "        .where(F.col('hourly_earn') > 0)\\\n",
    "        .where(F.col('total_amount') > 0)\\\n",
    "        .where(F.col('congestion_surcharge') >= 0)\\\n",
    "        .where(F.col('passenger_count') >= 0)\\\n",
    "        .where(F.col('payment_type') == 1)\\\n",
    "        .dropna()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../data/curated/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of raw data : 19585935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null data ratio:  0.9716568037216503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Num of raw data :', sdf.count())\n",
    "print('Null data ratio: ', sdf.dropna().count()/sdf.count())\n",
    "#Derive the percentage of instances without null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = new_feature(sdf)\n",
    "sdf = feature_clean(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 00:34:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 26:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data after being filtered:  15121628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Num of data after being filtered: ', sdf.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge New Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset = spark.read.csv(\"../data/newyork_weather.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>datetime</th><th>temp</th><th>precip</th><th>windspeedmean</th><th>visibility</th></tr>\n",
       "<tr><td>2022-12-01</td><td>18.9</td><td>0</td><td>7</td><td>4.1</td></tr>\n",
       "<tr><td>2022-12-02</td><td>19.7</td><td>0</td><td>7.6</td><td>5</td></tr>\n",
       "<tr><td>2022-12-03</td><td>20.7</td><td>0</td><td>4.9</td><td>4.7</td></tr>\n",
       "<tr><td>2022-12-04</td><td>21</td><td>0</td><td>6.2</td><td>4.4</td></tr>\n",
       "<tr><td>2022-12-05</td><td>22</td><td>0</td><td>9.3</td><td>5.3</td></tr>\n",
       "<tr><td>2022-12-06</td><td>20.6</td><td>0</td><td>8.5</td><td>4.3</td></tr>\n",
       "<tr><td>2022-12-07</td><td>19.1</td><td>0</td><td>10</td><td>6.5</td></tr>\n",
       "<tr><td>2022-12-08</td><td>18.1</td><td>0</td><td>9.5</td><td>7</td></tr>\n",
       "<tr><td>2022-12-09</td><td>18.1</td><td>0</td><td>11.4</td><td>5.4</td></tr>\n",
       "<tr><td>2022-12-10</td><td>18.5</td><td>0</td><td>9.4</td><td>6.8</td></tr>\n",
       "<tr><td>2022-12-11</td><td>21</td><td>0</td><td>11.2</td><td>4.9</td></tr>\n",
       "<tr><td>2022-12-12</td><td>23.2</td><td>0</td><td>9.1</td><td>7.2</td></tr>\n",
       "<tr><td>2022-12-13</td><td>21.6</td><td>0</td><td>13.5</td><td>5.1</td></tr>\n",
       "<tr><td>2022-12-14</td><td>21.5</td><td>0</td><td>14.2</td><td>6.5</td></tr>\n",
       "<tr><td>2022-12-15</td><td>21.8</td><td>3</td><td>6.1</td><td>5.3</td></tr>\n",
       "<tr><td>2022-12-16</td><td>22.8</td><td>0</td><td>7.2</td><td>4.7</td></tr>\n",
       "<tr><td>2022-12-17</td><td>23</td><td>0</td><td>7.2</td><td>7.8</td></tr>\n",
       "<tr><td>2022-12-18</td><td>21.1</td><td>0</td><td>9.5</td><td>8.4</td></tr>\n",
       "<tr><td>2022-12-19</td><td>20.1</td><td>0</td><td>7.2</td><td>7.5</td></tr>\n",
       "<tr><td>2022-12-20</td><td>20.2</td><td>0</td><td>6.6</td><td>7</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+----+------+-------------+----------+\n",
       "|  datetime|temp|precip|windspeedmean|visibility|\n",
       "+----------+----+------+-------------+----------+\n",
       "|2022-12-01|18.9|     0|            7|       4.1|\n",
       "|2022-12-02|19.7|     0|          7.6|         5|\n",
       "|2022-12-03|20.7|     0|          4.9|       4.7|\n",
       "|2022-12-04|  21|     0|          6.2|       4.4|\n",
       "|2022-12-05|  22|     0|          9.3|       5.3|\n",
       "|2022-12-06|20.6|     0|          8.5|       4.3|\n",
       "|2022-12-07|19.1|     0|           10|       6.5|\n",
       "|2022-12-08|18.1|     0|          9.5|         7|\n",
       "|2022-12-09|18.1|     0|         11.4|       5.4|\n",
       "|2022-12-10|18.5|     0|          9.4|       6.8|\n",
       "|2022-12-11|  21|     0|         11.2|       4.9|\n",
       "|2022-12-12|23.2|     0|          9.1|       7.2|\n",
       "|2022-12-13|21.6|     0|         13.5|       5.1|\n",
       "|2022-12-14|21.5|     0|         14.2|       6.5|\n",
       "|2022-12-15|21.8|     3|          6.1|       5.3|\n",
       "|2022-12-16|22.8|     0|          7.2|       4.7|\n",
       "|2022-12-17|  23|     0|          7.2|       7.8|\n",
       "|2022-12-18|21.1|     0|          9.5|       8.4|\n",
       "|2022-12-19|20.1|     0|          7.2|       7.5|\n",
       "|2022-12-20|20.2|     0|          6.6|         7|\n",
       "+----------+----+------+-------------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sdf = sdf.join(weather_dataset, on = 'datetime', how = 'leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming them to float datatype from string\n",
    "merged_sdf = merged_sdf.withColumn(\"temp\", F.col(\"temp\").cast('float'))\n",
    "merged_sdf = merged_sdf.withColumn(\"precip\", F.col(\"precip\").cast('float'))\n",
    "merged_sdf = merged_sdf.withColumn(\"windspeedmean\", F.col(\"windspeedmean\").cast('float'))\n",
    "merged_sdf = merged_sdf.withColumn(\"visibility\", F.col(\"visibility\").cast('float'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataset to parquet file ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_sdf.write.mode('overwrite').parquet('../data/curated/merged_sdf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengqian/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/Users/chengqian/Library/Python/3.9/lib/python/site-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "# 1% instances from the original dataset as sample\n",
    "df = merged_sdf.sample(0.01, seed=0).toPandas()\n",
    "df.to_parquet('../data/curated/sample_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
